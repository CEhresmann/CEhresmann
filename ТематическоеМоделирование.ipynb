{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CEhresmann/CEhresmann/blob/master/%D0%A2%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5%D0%9C%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgBtKODsNUQ7"
      },
      "source": [
        "# Фанфикшн и тематическое моделирование\n",
        "## Почему фанфикшн?\n",
        "Хотя фанфикшн (фанатская литература) появилась еще до появления Интернета, распространяясь в небольших фанатских изданиях, сейчас сложно представить этот вид литературы (некоторые исследователи называют это паралитературой, другие считают разновидностью постфольклора) вне Интернета. Это делает его очень привлекательным для анализа - тексты уже сразу же рождаются в цифровом формате, а также, как правило, снабжены так называемой \"шапкой\" - метаданными. Эти метаданные сообщают фандом, возрастной рейтинг, направленность (например, Слэш - гомосексуальные отношения). Эти метаданные интересны, так как создаются самими участниками сообщества фикрайтеров.\n",
        "## Что мы делаем?\n",
        "Предположим, что мы хотим узнать как связаны часть такой шапки - жанр - с содержанием самих текстов. Для начала нам надо собрать корпус текстов с метаданными. Корпус можно балансировать по разным принципам, в нашем случае мы берем по сто текстов четырех направленностой для каждого из четырех жанров (Ангст, Флафф, Фэнтези и Повседневность), всего 1600 текстов. Важно понимать, что каждый из этих текстов относится и к другим жанрам (как правило все тексты имеют больше одной жанровой метки), в нашем случае важно, что тексты в наших четырех жанрах не пересекаются между собой (то есть, мы исключили тексты, присутсвующие в более чем одном из этих четырех жанров).\n",
        "## Для чего нужно тематическое моделирование?\n",
        "Как же атоматически определить содержание этих текстов? Одним из популярных способов для этого является тематическое моделирование. По существу, оно позволяет выделять из текстов темы, с какой-то вероятностью порождающие слова и затем смотреть, с какую вероятностью тексты соотносятся с этими темами. Для нашей задачи мы будем применять тематическое моделирование, основанное на Латентном размещении Дирихле ([подробнее прочитать про это можно здесь](https://sysblok.ru/knowhow/kak-ponjat-o-chem-tekst-ne-chitaja-ego/)). Так как нужные алгоритмы уже имплементированы в Питоне, математика, стоящая за ними, нас беспокоить не будет."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVBaFjL1Hyb0"
      },
      "source": [
        "Датасет с текстами можно найти на этом [гугл-диске](https://drive.google.com/drive/folders/1uEnZJDzty2u0h9O1pdhPHcUIzeWss8xI?usp=sharing). Функцию для скачивания и другие материалы можно увидеть по [ссылке](https://github.com/makarfedorov/topic_modeling_paraliterature). Для того, чтобы код заработал, необходимо загрузить датасет к себе на диск и заменить путь к файлу в переменной adress на путь к файлу на своем диске. Следует заметить, что pandas не является стандартной библиотекой для Питона и его следует устанавливать, если вы не работаете в Колабе - здесь эта библиотека уже предустановлена"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ls7rcE-K1g3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "8113a059-7194-4c1f-f65e-0de4ca595a9f"
      },
      "source": [
        "!pip install pyldavis\n",
        "import os\n",
        "import pandas as pd\n",
        "address = \"/content/drive/MyDrive/ficbook_one_file/corpus_fanfic.csv\"\n",
        "fanfic_data = pd.read_csv(adress)\n",
        "os.chdir(\"/content\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyldavis in /usr/local/lib/python3.12/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from pyldavis) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pyldavis) (1.16.3)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyldavis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from pyldavis) (1.5.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from pyldavis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from pyldavis) (2.14.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.12/dist-packages (from pyldavis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pyldavis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (from pyldavis) (4.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from pyldavis) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyldavis) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyldavis) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyldavis) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim->pyldavis) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->pyldavis) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim->pyldavis) (2.0.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'adress' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3410370604.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maddress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/ficbook_one_file/corpus_fanfic.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfanfic_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'adress' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uVOvQXoUs7s"
      },
      "source": [
        "Посмотрим на наши данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31hTHcRblevF"
      },
      "source": [
        "fanfic_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FYXpJoPOCIT"
      },
      "source": [
        "Для начала загрузим токенайзер и пунктуацию из библиотеки **nltk**. Кроме этого нам понадобится список стоп-слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bg4uV7XNgAU"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import download as nltk_download\n",
        "\n",
        "nltk_download(\"punkt\")\n",
        "!wget https://raw.githubusercontent.com/dhhse/dh2020/master/data/stop_ru.txt\n",
        "with open (\"stop_ru.txt\", \"r\") as stop_ru:\n",
        "    rus_stops = [word.strip() for word in stop_ru.readlines()]\n",
        "punctuation = '!\\\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~—»«...–'\n",
        "filter = rus_stops + list (punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HMiibm3Og02"
      },
      "source": [
        "Теперь установим лемматизатор, для этой цели подойдет библиотека **pymorphy2**. С помощью нее можно приводить слова в начальную форму (программа может приводить в том числе те слова, которые ей \"незнакомы\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL5KsjK-NmoZ"
      },
      "source": [
        "!pip install pymorphy2\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "parser = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjPMvlybPNoT"
      },
      "source": [
        "Напишем функцию для предобработки текста. Слова приводятся к нижнему регистру, стоп-слова удаляются, далее слова лемматизируются\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtl4ovZlNu_u"
      },
      "source": [
        "def preprocess(input_text):\n",
        "    \"\"\"\n",
        "    Функция для предобработки текста. Слова приводятся к нижнему регистру,\n",
        "    стоп-слова удаляются, далее слова лемматизируются\n",
        "    :param input_text: Входной текст для очистки и лемматизации\n",
        "    :return: Очищенный и лемматизированный текст\n",
        "    \"\"\"\n",
        "    text = input_text.lower()\n",
        "    tokenized_text = word_tokenize(text)\n",
        "    clean_text = [word for word in tokenized_text if word not in filter]\n",
        "    lemmatized_text = [parser.parse(word)[0].normal_form for word in\n",
        "                       clean_text]\n",
        "\n",
        "    return lemmatized_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTC8w2tRPd4g"
      },
      "source": [
        "Теперь можно применить эту функцию к нашему датасету с текстами.Это лучше всего сделать с помощью функции **map** (или **apply**), которая может применить созданную нами функцию к каждому полю с текстом в таблице"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp4pM1kYN2de"
      },
      "source": [
        "fanfic_data[\"text_processed\"] = fanfic_data[\"text\"].map(preprocess)\n",
        "fanfic_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AGtGtEoPKab"
      },
      "source": [
        "Перед тем как делать собственно тематическое моделирование стоит посмотреть какие фандомы есть в дата-сете. Видно, что, во-первых, там есть отдельные фандомы для книг/фильмов по ним. Во-вторых, есть много разных комбинаций из фандомов франшизы Марвел (отдельный фандом для \"Первого мстителя\", \"Железного человека\" и т.д.)  и тому подобного."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXWyHuxaepl8"
      },
      "source": [
        "fanfic_data[\"fandom\"].unique()\n",
        "# Роулинг Джоан «Гарри Поттер»,Гарри Поттер', Железный человек,Мстители,Человек-паук: Возвращение домой,  Вдали от дома', 'Железный человек,Первый мститель,Человек-паук: Возвращение домой,  Вдали от дома\n",
        "# 'Железный человек,Первый мститель,Доктор Стрэндж,Человек-паук: Возвращение домой,  Вдали от дома',"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj77uxV1R9E9"
      },
      "source": [
        "Все эти фандомы необходимо стандартизовать, иначе их получится слишком много для исследования их связи с темами. Это можно сделать выбрав одну название для группы фандомов, которые мы будем считать единым фандомов, и просто заменяя все название на одно. Например, \"Властелин колец\", \"Толкин Джон Р.Р. «Властелин колец»\" и \"Хоббит\" становятся просто \"Властелином Колец\". Далее можно посмотреть 10 самых частотных фандомов, которые останутся как есть. Остальные низкочастотные фандомы будут помечены либо как \"прочее\" (если таковой присутствует отдельно), либо как \"кроссовер\" (если там несколько фандомов). Важно понимать, что функция для стандартизации фандомов работает только с конкетными фандомами, которые есть в нашем датасете. Для другого датасета пришлось бы делать иную стандартизацию (даже если набор фандомов сильно не отличается, их частотности могут быть другими, а значит, список из 10 самых частотных фандомов может быть другим)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAGK5dWpf5dR"
      },
      "source": [
        "def standart(fandoms):\n",
        "    \"\"\"\n",
        "    Функция для стандартизации фандомов\n",
        "    :param fandoms: исходный список фандомов\n",
        "    :return: стандартизованный фандом\n",
        "    \"\"\"\n",
        "    fandom_dictionary = {\"Роулинг Джоан «Гарри Поттер»\":\"Гарри Поттер\",\n",
        "                         \"Первый мститель\":\"Марвел\",\n",
        "                         \"Железный человек\":\"Марвел\",\n",
        "                         \"Человек-паук: Возвращение домой,  Вдали от дома\":\"Марвел\",\n",
        "                         \"Доктор Стрэндж\":\"Марвел\",\n",
        "                         \"Толкин Джон Р.Р. «Властелин колец»\":\"Властелин Колец\",\n",
        "                         \"Тор\":\"Марвел\",\n",
        "                         \"Толкин Джон Р. Р. «Хоббит, или Туда и обратно»\":\"Властелин Колец\",\n",
        "                         \"Капитан Марвел\":\"Марвел\",\n",
        "                         \"Сапковский Анджей «Ведьмак» (Сага о ведьмаке)\":\"Ведьмак\",\n",
        "                         \"The Witcher\":\"Ведьмак\",\n",
        "                         \"Мартин Джордж «Песнь Льда и Пламени»\":\"Игра Престолов\",\n",
        "                         \"Boruto: Naruto Next Generations\":\"Naruto\",\n",
        "                         \"Коллинз Сьюзен «Голодные игры»\":\"Голодные игры\",\n",
        "                         \"Удивительный Человек-паук\":\"Марвел\",\n",
        "                         \"Доктор Стрэндж и тайна Ордена магов\":\"Марвел\",\n",
        "                         \"Фантастические твари\":\"Гарри Поттер\",\n",
        "                         \"Marvel Comics\":\"Марвел\",\"Тор,Доктор Стрэндж\":\"Марвел\",\n",
        "                         \"Мстители,Доктор Стрэндж\":\"Марвел\",\n",
        "                         \"Дэдпул,Дэдпул\":\"Марвел\",\n",
        "                         \"Черная Пантера\":\"Марвел\", \"  Вдали от дома\":\"Марвел\",\n",
        "                         \"Мстители\":\"Марвел\", \"Новый Человек-паук\":\"Марвел\",\n",
        "                         \"Человек-паук: Возвращение домой\":\"Марвел\",\n",
        "                         \"Человек-Паук\":\"Марвел\", \"Хоббит\":\"Властелин Колец\",\n",
        "                         \"Человек-паук: Через Вселенные\":\"Марвел\",\n",
        "                         \"Человек-паук\":\"Марвел\", \"Веном\":\"Марвел\",\n",
        "                         \"Стражи Галактики\":\"Марвел\", \"Дэдпул\":\"Марвел\",\n",
        "                         \"Черная вдова\":\"Марвел\", \"Халк\":\"Марвел\"}\n",
        "    sig_list = [\"Ориджиналы\", \"Гарри Поттер\", \"Марвел\",\n",
        "                \"Чудесная божья коровка (Леди Баг и Супер-Кот)\",\n",
        "                \"Bangtan Boys (BTS)\",\n",
        "                \"Однажды в сказке\",\"Fairy Tail\", \"Сотня\", \"Naruto\", \"Волчонок\"]\n",
        "\n",
        "    standart_fandoms = []\n",
        "    fandoms = fandoms.split(\",\")\n",
        "    for fandom in fandoms:\n",
        "        if fandom in fandom_dictionary:\n",
        "            fandom = fandom_dictionary[fandom]\n",
        "        if fandom in sig_list:\n",
        "            standart_fandoms.append(fandom)\n",
        "        else:\n",
        "            standart_fandoms.append(\"Прочее\")\n",
        "\n",
        "    final = list(set(standart_fandoms))\n",
        "    if len(final) > 1:\n",
        "        final = \"Кроссовер\"\n",
        "    else:\n",
        "        final = final[0]\n",
        "\n",
        "    return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9vUmKWrTXFC"
      },
      "source": [
        "Можно посмотреть на количество стандартизированных фандомов и, на всякий случай, сохранить их в отдельный файл"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPBeaRRW_j9e"
      },
      "source": [
        "fanfic_data[\"standart_fandom\"] = fanfic_data[\"fandom\"].apply(standart)\n",
        "fandoms = fanfic_data[\"standart_fandom\"].value_counts()\n",
        "print(fandoms)\n",
        "fandoms.to_csv(\"fandoms3.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2rU7i5sPi8P"
      },
      "source": [
        "Надо импортировать библиотеку **gensim**. Затем мы создаем словарь для тематического моделирования из лемматизированного текста. После создания словаря лучше всего отфлильтровать те слова, которые встречаются в слишком большом количестве текстов, и те, которые встречаются в слишком маленьком количество текстов. Для этого есть метод **filter_extremes**, который принимает в себя аргументы **no_above** (только слова, которые встречаются не более, чем в указанной доле текстов) и **no_below=20** (слова, которые встречаются не менее чем в указанном количестве текстов). После удаления лищних слов, словарь лучше всего ужать в размерах, убрав пропуски с помощью метода **compactify**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz1Gn1QpOVFh"
      },
      "source": [
        "import gensim\n",
        "gensim_dictionary = gensim.corpora.Dictionary(fanfic_data[\"text_processed\"])\n",
        "gensim_dictionary.filter_extremes(no_above=0.1, no_below=20)\n",
        "gensim_dictionary.compactify()\n",
        "\n",
        "print(gensim_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iEZArKpWTRS"
      },
      "source": [
        "**Теперь** создаем корпус в виде \"мешка слов\" (bag of words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH8b-chJOXdl"
      },
      "source": [
        "corpus = [gensim_dictionary.doc2bow(text)\n",
        "          for text in fanfic_data['text_processed']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjeVZ5iUcZ9M"
      },
      "source": [
        "Теперь можно сделать само тематическое моделирование. Для этого, помимо созданного корпуса и словаря, необходимо указать количество \"обходов\", которые будет делать алгоритм (чем больше, тем точнее и медленнее сооздаваться будет модель) и количество тем, которые мы хотим выделить. Пока возьмем 20 тем (еще стоит не забыть установить **random_state** на какое-нибудь число - это позволит восстановить результат)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILF2zu0lOdpq"
      },
      "source": [
        "lda_20 = gensim.models.LdaMulticore(corpus,\n",
        "                                 num_topics=20,\n",
        "                                 id2word=gensim_dictionary,\n",
        "                                 passes=10, random_state=6457)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKzSkGWyPx4A"
      },
      "source": [
        "Тематическое моделирование для 20 тем (с помощью Латентного размещения Дирихле)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_B4OMSeOmSx"
      },
      "source": [
        "lda_20.print_topics()\n",
        "\n",
        "fw = open(\"topics_20.txt\", \"w\", encoding=\"utf-8\")\n",
        "for topic in lda_20.print_topics():\n",
        "    fw.write(str(topic))\n",
        "    print(str(topic))\n",
        "fw.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiCJONjRQNFh"
      },
      "source": [
        "Для того, чтобы узнать, какое количество топиков оптимально, можно использовать метрики, встроенные в библиотеку **gensim** - например, **c_v** или **c_uci**.Посмотрим, какое значение **c_v** есть дя модели на 20 тем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFpELquM_5hQ"
      },
      "source": [
        "from gensim.models import CoherenceModel\n",
        "coherence_model_lda = CoherenceModel(model=lda_20,\n",
        "                                     texts=fanfic_data[\"text_processed\"],\n",
        "                                     dictionary=gensim_dictionary,\n",
        "                                     coherence=\"c_v\")\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "\n",
        "print(\"\\nCoherence Score: \", coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQE6heWCl7yI"
      },
      "source": [
        "Значение метрики важно не только само по себе, но и как сравнение с другими возможными количествами тем. Создадим график, пусть на оси х будут отложено количество тем, а на оси у - соответствующее значение метрики. Для этого сначала надо сделать список значений соответвующей метрики для какого-нибудь промежутка тем (для экономии времени лучше вычислять это не для каждого значения количества тем, вполне можно взять шаг в три темы)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGpdw8WuRVbN"
      },
      "source": [
        "def coherence_score(dictionary, corpus, texts, max, start=2, step=3,\n",
        "                    measure=\"c_uci\"):\n",
        "    \"\"\"\n",
        "    Функция вычисляет метрики для оценки тем. моделирования и выводит\n",
        "    график, где по оси x отложено количество топиков, а по оси y - значение\n",
        "    метрики\n",
        "    :param dictionary: словарь для тематического моделирования\n",
        "    :param corpus: корпус в виде мешка слов\n",
        "    :param texts: тексты документов\n",
        "    :param max: максимальное количество топиков\n",
        "    :param start: стартовое количество топиков\n",
        "    :param step: промежуток, с которым вычисляются топики\n",
        "    :param measure: метрика\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    for num_topics in range(start, max, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary,\n",
        "                                           passes=10, num_topics=num_topics,\n",
        "                                           random_state=6457)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts,\n",
        "                                        dictionary=dictionary,\n",
        "                                        coherence=measure)\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    x = range(start, max, step)\n",
        "    plt.plot(x, coherence_values)\n",
        "    plt.xlabel(\"Number of Topics\")\n",
        "    plt.ylabel(measure + \"score\")\n",
        "    plt.legend((\"coherence_score\"), loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9P1fS6DzgqJ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "coherence_score(dictionary=gensim_dictionary, corpus=corpus, texts=fanfic_data[\"text_processed\"], start=2, max=30, step=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7m4sRtm748d"
      },
      "source": [
        "coherence_score(dictionary=gensim_dictionary, corpus=corpus, texts=fanfic_data[\"text_processed\"], start=2, max=30, step=3, measure=\"c_v\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQiaf0zTR3vy"
      },
      "source": [
        "На обоих графиках виден совпадающий пик примерно на 20 темах. Тем не менее, если посмотреть на визуализацию расстояние между топиками, то видно, что 20 топиков сильно накладываются друг на друга. Далее будет видно, что 10 тем накладываются менее существенно, поэтому дальнейшая часть работы дублируется для 20 и для 10 топиков."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNEQ_ya5I2TZ"
      },
      "source": [
        "*В* библиотеке **gensim** есть встроенная интерактивная визаулизация расстояния между темами. Можно использовать ее для того, чтобы оценить насколько пересекаются между собой полученные темы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LidpXv1EeLmw"
      },
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uue9g1xTehay"
      },
      "source": [
        "vis_20 = gensimvis.prepare(lda_20, corpus, gensim_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKmgT7c0jG22"
      },
      "source": [
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4ab9c8KjJie"
      },
      "source": [
        "vis_20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygNWCUs3S-uh"
      },
      "source": [
        "Видно, что большинство тем находятся очень близко друг к другу. Возможно, стоит посмотреть на модель с другим количеством тем, например, с 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iPyLsUhzi51"
      },
      "source": [
        "lda_10 = gensim.models.LdaMulticore(corpus,\n",
        "                                    num_topics=10,\n",
        "                                    id2word=gensim_dictionary,\n",
        "                                    passes=10, random_state=6457)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITVOrDeHHk7w"
      },
      "source": [
        "vis_10 = gensimvis.prepare(lda_10, corpus, gensim_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn_y7k_nHpXW"
      },
      "source": [
        "vis_10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkH4kwO2TQYc"
      },
      "source": [
        "Темы уже не налезают друг на друга так сильно. Можно использовать обе модели - на 10 и на 20 тем, дальнейшие операции будут для них одинаковы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzXbukVKO5Hp"
      },
      "source": [
        "Теперь необходимо назначить каждому документу в нашем копусе наиболее подходящую (наиболее вероятную) для него тему. Для удобства лучше разделить тему документа и ее вероятность в разные колонки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Ty1ps2xtho"
      },
      "source": [
        "def get_topic(words, lda):\n",
        "    \"\"\"\n",
        "    Функция назначает документу наиболее вероятный топик\n",
        "    :param words: лемматизированный текст документа\n",
        "    :param lda: тематическая модель\n",
        "    :return: список из наиболее вероятного топика\n",
        "    и его вероятности\n",
        "    \"\"\"\n",
        "    bag = lda.id2word.doc2bow(words)\n",
        "    topics = lda.get_document_topics(bag)\n",
        "    topic_dictionary = {}\n",
        "    for topic in topics:\n",
        "        topic_dictionary[topic[1]] = str((topic[0]))\n",
        "    main_probability = max(topic_dictionary)\n",
        "    main_topic = topic_dictionary[main_probability]\n",
        "\n",
        "    return [main_topic, main_probability]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbC3XwIwBsqE"
      },
      "source": [
        "fanfic_data[\"lda_20\"] = fanfic_data[\"text_processed\"].apply(get_topic,\n",
        "                                                            lda=lda_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq4vESZODk9m"
      },
      "source": [
        "fanfic_data[\"topic_20\"] = fanfic_data[\"lda_20\"].str[0]\n",
        "fanfic_data[\"probability_20\"] = fanfic_data[\"lda_20\"].str[1]\n",
        "del fanfic_data[\"lda_20\"]\n",
        "fanfic_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6OkbTdg0oI8"
      },
      "source": [
        "fanfic_data[\"lda_10\"] = fanfic_data[\"text_processed\"].apply(get_topic,\n",
        "                                                            lda=lda_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2b5uHA11O2Q"
      },
      "source": [
        "fanfic_data[\"topic_10\"] = fanfic_data[\"lda_10\"].str[0]\n",
        "fanfic_data[\"probability_10\"] = fanfic_data[\"lda_10\"].str[1]\n",
        "del fanfic_data[\"lda_10\"]\n",
        "\n",
        "fanfic_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tmctYkmEwbX"
      },
      "source": [
        "fanfic_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Q1W4s3PumO"
      },
      "source": [
        "Теперь можно перейти к самому ответу на вопрос, как связаны жанр и фандом фанфика с его содержанием. Построим график, где по горизонтале отложены жанры,а по вертикали - темы. ДЛя этого импортируем библиотеку **seaborn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBFw6IxVw7dM"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.catplot(x=\"genre\", y=\"topic_20\", kind=\"swarm\", data=fanfic_data, height=5,\n",
        "            aspect=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ6hPUMj3CW1"
      },
      "source": [
        "sns.catplot(x=\"genre\", y=\"topic_10\", kind=\"swarm\",data=fanfic_data, height=5,\n",
        "            aspect=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyI222j38Hfv"
      },
      "source": [
        "Видно, что жанры распределяются по темам более-менее равномерно. Теперь посмотрим, как соотносятся с темами стандартизованные фандомы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iKfsjZTA2lR"
      },
      "source": [
        "sns.catplot(x=\"standart_fandom\", y=\"topic_20\", kind=\"strip\", data=fanfic_data,\n",
        "            height=5, aspect=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuf6hLXl7XCo"
      },
      "source": [
        "sns.catplot(x=\"standart_fandom\", y=\"topic_10\", kind=\"strip\", data=fanfic_data,\n",
        "            height=5, aspect=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEqf9NbT8alc"
      },
      "source": [
        "Видно, что, во-первых, фандом \"ориджиналы\" и \"прочее\" (низкочастотные фандомы) распределяются по темам равномернее всего. Остальные фандомы преобладающе связаны с одной-двумя темами. Далее можно посмотрить и связь возрастного рейтинга и направленности фанфиков с темами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmfQxiMrgWm_"
      },
      "source": [
        "sns.catplot(x=\"rating\", y=\"topic_20\", kind=\"swarm\", data=fanfic_data, height=5,\n",
        "            aspect=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELKaChsb7ikL"
      },
      "source": [
        "sns.catplot(x=\"rating\", y=\"topic_10\", kind=\"swarm\", data=fanfic_data, height=5,\n",
        "            aspect=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S82e9lTUgy9C"
      },
      "source": [
        "sns.catplot(x=\"romance\", y=\"topic_20\", kind=\"swarm\", data=fanfic_data,\n",
        "            height=5, aspect=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGvmGKQb97lb"
      },
      "source": [
        "Среди тем можно было заметить одну, по всей видимости связанную с жанром \"омегаверс\". Вот как \"Книга фанфиков\" определяет этот жанр:“Действие работы происходит в мире, где персонажи распределены на три типа: альфы, беты и омеги, каждый из которых обладает рядом физиологических особенностей, сексуальных пристрастий, определенным положением в социальной иерархии и т.д.”.   А вот тема  среди 10 тем, которая очень на него напоминает:   **('0.021*\"альфа\" + 0.020*\"омег\" + 0.019*\"альф\" + 0.018*\"омега\" + 0.008*\"течка\" + 0.007*\"бета\" + 0.007*\"виктор\" + 0.004*\"крис\" + 0.004*\"гермиона\" + 0.003*\"марк\"')**. Фанфики этого жанра имееют указание на него среди своих меток. Интересно проверить, действительно ли омегаверс связан с определенной темой (темами). Для того, чтобы этого выяснить, сначала нужно выделить метку Омегаверса в отдельную колонку (присутствует ли она среди меток фанфика или нет)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fLXQFTonwGU"
      },
      "source": [
        "def omega(data):\n",
        "    \"\"\"\n",
        "    Функция выдает строку Омегаверс, если Омегаверс присутствует\n",
        "    в списке тегов и строку Не омегаверс, если отсутствует\n",
        "    :param data: список тегов\n",
        "    :return: строка Омегаверс или строка Не омегаверс\n",
        "    \"\"\"\n",
        "    omega = \"\"\n",
        "    if \"Омегаверс\" in data:\n",
        "        omega = \"Омегаверс\"\n",
        "    else:\n",
        "        omega = \"Не омегаверс\"\n",
        "    return omega"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoQWexJvnYNa"
      },
      "source": [
        "fanfic_data[\"Omega\"] = fanfic_data[\"tag\"].apply(omega)\n",
        "fanfic_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_K9jZyoMSSy"
      },
      "source": [
        "Теперь можно построить график и посмотреть, как распределяются фанфики омагеверс и не омегаверс относительно тем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tamTZg4bof-_"
      },
      "source": [
        "sns.catplot(x=\"Omega\", y=\"topic_10\", kind=\"strip\", data=fanfic_data, height=5,\n",
        "            aspect=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft5O5eUoMo06"
      },
      "source": [
        "sns.catplot(x=\"Omega\", y=\"topic_20\", kind=\"strip\", data=fanfic_data, height=5,\n",
        "            aspect=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFPzgjR0Mulg"
      },
      "source": [
        "Видно, что омегаверс преимущественно связан с несколькими темами. Можно сделать вывод, что, хотя те жанры, которые мы выбрали, мало связаны с конкретными темами, есть жанры, которые с ними связаны. Важно сделать уточнение - если посмотреть на выделившиеся темы, то легко увидеть, насколько много там встречаются имен. В дальнейшем, было бы полезно сделать тематическое моделирование, предварительно удалив именные сущности из текста, но здесь мы пока заниматься этим не будем"
      ]
    }
  ]
}